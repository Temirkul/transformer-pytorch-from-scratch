{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformer components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SelfAttention(nn.Module):\n",
    "    def __init__(self, embed_dim: int, n_heads: int):\n",
    "        super().__init__()\n",
    "        self.embed_dim = embed_dim\n",
    "        self.n_heads = n_heads\n",
    "        self.head_dim = embed_dim // n_heads\n",
    "\n",
    "        assert (\n",
    "            self.head_dim * n_heads == embed_dim\n",
    "        ), \"embed_dim needs to be divisible by n_heads\"\n",
    "\n",
    "        self.queries = nn.Linear(self.embed_dim, self.embed_dim, bias=False)\n",
    "        self.keys = nn.Linear(self.embed_dim, self.embed_dim, bias=False)\n",
    "        self.values = nn.Linear(self.embed_dim, self.embed_dim, bias=False)\n",
    "        self.fc_out = nn.Linear(self.embed_dim, self.embed_dim)\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        pre_queries: torch.Tensor,\n",
    "        pre_keys: torch.Tensor,\n",
    "        pre_values: torch.Tensor,\n",
    "        mask: torch.Tensor,\n",
    "    ) -> torch.Tensor:  # shapes of pre_query, pre_key, pre_value are [N, seq_len, embed_dim]. They have the 'pre' prefix because they're the inputs (word vectors) to get the queries, keys, and values\n",
    "        N = pre_queries.shape[0]  # batch size\n",
    "        queries_seq_len, keys_seq_len, values_seq_len = pre_queries.shape[1], pre_keys.shape[1], pre_values.shape[1]  # sequence (sentence) lengths which may be different in encoder/decoder\n",
    "\n",
    "        queries = self.queries(pre_queries)\n",
    "        keys = self.keys(pre_keys)\n",
    "        values = self.values(pre_values)\n",
    "\n",
    "        queries = queries.reshape(N, queries_seq_len, self.n_heads, self.head_dim)\n",
    "        keys = keys.reshape(N, keys_seq_len, self.n_heads, self.head_dim)\n",
    "        values = values.reshape(N, values_seq_len, self.n_heads, self.head_dim)\n",
    "\n",
    "        attention_grid = torch.einsum(\"nqhd, nkhd -> nhqk\", queries, keys)  # shape is [N, self.n_heads, queries_seq_len, keys_seq_len], we sum over self.head_dim\n",
    "\n",
    "        if mask is not None:\n",
    "            attention_grid = attention_grid.masked_fill(mask == 0, float(\"-1e20\"))\n",
    "\n",
    "        attention_weights = torch.softmax(attention_grid / self.head_dim ** (1 / 2), dim=3)  # dim=3 because they need to sum up to one along the key dimension (cuz query asks keys intuitively)\n",
    "\n",
    "        multi_head = torch.einsum(\"nhql, nlhd -> nqhd\", attention_weights, values)  # this computes all the heads which now need to be concatenated. Shape is [N, query_seq_len, self.n_heads, self.head_dim]\n",
    "        multi_head_attention = multi_head.reshape(N, queries_seq_len, self.embed_dim)  # concatenation, notice that it's the attention vectors for each corresponding query\n",
    "        multi_head_attention = self.fc_out(multi_head_attention)  # final weight matrix multiplication\n",
    "\n",
    "        return multi_head_attention  # shape is [N, queries_seq_len, embed_dim], i.e. the attention vector for each corresponding query vector\n",
    "\n",
    "\n",
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(\n",
    "        self, embed_dim: int, n_heads: int, dropout: float, forward_expansion: int\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.attention = SelfAttention(embed_dim, n_heads)\n",
    "        self.norm = nn.LayerNorm(embed_dim)\n",
    "        self.linear_1 = nn.Linear(embed_dim, forward_expansion * embed_dim)\n",
    "        self.linear_2 = nn.Linear(forward_expansion * embed_dim, embed_dim)\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        pre_queries: torch.Tensor,\n",
    "        pre_keys: torch.Tensor,\n",
    "        pre_values: torch.Tensor,\n",
    "        mask: torch.Tensor,\n",
    "    ) -> torch.Tensor:\n",
    "        attention = self.attention(pre_queries, pre_keys, pre_values, mask)\n",
    "        x = self.dropout(self.norm(attention + pre_queries))  # initially, the pre_queries, pre_keys, pre_values are either the same or different if in the decoder, they're just the word vectors\n",
    "        x_fc = self.linear_2(F.gelu(self.linear_1(x)))\n",
    "        out = self.dropout(self.norm(x + x_fc))\n",
    "        return out  # shape is [N, queries_seq_len, embed_dim]\n",
    "\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        src_vocab_size: int,\n",
    "        embed_dim: int,\n",
    "        n_layers: int,\n",
    "        n_heads: int,\n",
    "        device: torch.device,\n",
    "        forward_expansion: int,\n",
    "        dropout: float,\n",
    "        max_seq_length: int,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.device = device\n",
    "        self.word_embedding = nn.Embedding(src_vocab_size, embed_dim)\n",
    "        self.positional_embedding = nn.Embedding(max_seq_length, embed_dim)\n",
    "        self.layers = nn.ModuleList(\n",
    "            [\n",
    "                TransformerBlock(embed_dim, n_heads, dropout, forward_expansion)\n",
    "                for _ in range(n_layers)\n",
    "            ]\n",
    "        )\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "    def forward(self, x: torch.Tensor, mask: torch.Tensor) -> torch.Tensor:\n",
    "        N, seq_length = x.shape  # seq_length == max_seq_length?\n",
    "        positions = torch.arange(0, seq_length).expand(N, -1).to(self.device)\n",
    "        out = self.dropout(self.word_embedding(x) + self.positional_embedding(positions))  # these are the word vectors\n",
    "        for layer in self.layers:\n",
    "            out = layer(out, out, out, mask)  # calling the forward method of the TransformerBlock\n",
    "        return out  # shape is [N, x_seq_len, embed_dim]\n",
    "\n",
    "\n",
    "class DecoderBlock(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        embed_dim: int,\n",
    "        n_heads: int,\n",
    "        forward_expansion: int,\n",
    "        dropout: float,\n",
    "        device: torch.device,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.norm = nn.LayerNorm(embed_dim)\n",
    "        self.attention = SelfAttention(embed_dim, n_heads)\n",
    "        self.transformer_block = TransformerBlock(embed_dim, n_heads, dropout, forward_expansion)\n",
    "        self.device = device\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        x: torch.Tensor,\n",
    "        keys: torch.Tensor,\n",
    "        values: torch.Tensor,\n",
    "        src_mask: torch.Tensor,\n",
    "        trg_mask: torch.Tensor,\n",
    "    ) -> torch.Tensor:  # src_mask is optional, it's so that we don't do computations on padded values. trg_mask is not optional, you MUST have it\n",
    "        attention = self.attention(x, x, x, trg_mask)\n",
    "        queries = self.dropout(self.norm(x + attention))  # the queries in the decoder which get passed into the second multiheaded attention\n",
    "        out = self.transformer_block(queries, keys, values, src_mask)\n",
    "        return out  # shape is [N, x_seq_len, embed_dim]\n",
    "\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        trg_vocab_size: int,\n",
    "        embed_dim: int,\n",
    "        n_layers: int,\n",
    "        n_heads: int,\n",
    "        device: torch.device,\n",
    "        forward_expansion: int,\n",
    "        dropout: float,\n",
    "        max_seq_length: int,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.device = device\n",
    "        self.word_embedding = nn.Embedding(trg_vocab_size, embed_dim)\n",
    "        self.positional_embedding = nn.Embedding(max_seq_length, embed_dim)\n",
    "        self.layers = nn.ModuleList(\n",
    "            [\n",
    "                DecoderBlock(embed_dim, n_heads, forward_expansion, dropout, device)\n",
    "                for _ in range(n_layers)\n",
    "            ]\n",
    "        )\n",
    "        self.fc_out = nn.Linear(embed_dim, trg_vocab_size)\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        x: torch.Tensor,\n",
    "        encoder_out: torch.Tensor,\n",
    "        src_mask: torch.Tensor,\n",
    "        trg_mask: torch.Tensor,\n",
    "    ) -> torch.Tensor:\n",
    "        N, seq_length = x.shape\n",
    "        positions = torch.arange(0, seq_length).expand(N, -1).to(self.device)\n",
    "        x = self.dropout(self.word_embedding(x) + self.positional_embedding(positions))\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, encoder_out, encoder_out, src_mask, trg_mask)  # encoder output is the attention vector which went through the feedforward nets & normalization + skip connection\n",
    "        out = self.fc_out(x)  # shape is [N, x_seq_len, trg_vocab_size]\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformer itself (putting it all together)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Translator(nn.Module):   \n",
    "    def __init__(\n",
    "        self, \n",
    "        src_vocab_size: int, \n",
    "        trg_vocab_size: int, \n",
    "        src_pad_idx: int, \n",
    "        trg_pad_idx: int, \n",
    "        embed_dim: int, \n",
    "        n_layers: int, \n",
    "        n_heads: int, \n",
    "        device: torch.device, \n",
    "        forward_expansion: int, \n",
    "        dropout: float, \n",
    "        max_seq_length: int\n",
    "    ):\n",
    "        super().__init__() \n",
    "        \n",
    "        self.encoder = Encoder(\n",
    "            src_vocab_size, \n",
    "            embed_dim, \n",
    "            n_layers, \n",
    "            n_heads, \n",
    "            device, \n",
    "            forward_expansion, \n",
    "            dropout, \n",
    "            max_seq_length\n",
    "        )\n",
    "        \n",
    "        self.decoder = Decoder(\n",
    "            trg_vocab_size, \n",
    "            embed_dim, \n",
    "            n_layers, \n",
    "            n_heads, \n",
    "            device, \n",
    "            forward_expansion, \n",
    "            dropout, \n",
    "            max_seq_length\n",
    "        )\n",
    "        \n",
    "        self.src_pad_idx = src_pad_idx\n",
    "        self.trg_pad_idx = trg_pad_idx\n",
    "        self.device = device \n",
    "        \n",
    "    def make_src_mask(self, src) -> torch.Tensor:  # src is of shape [N, src sentence length]\n",
    "        src_mask = (src != self.src_pad_idx).unsqueeze(1).unsqueeze(2)   \n",
    "        return src_mask.to(self.device)                                 \n",
    "         \n",
    "    def make_trg_mask(self, trg) -> torch.Tensor:\n",
    "        N, trg_length = trg.shape\n",
    "        trg_mask = torch.tril(torch.ones(trg_length, trg_length)).expand(N, 1, trg_length, trg_length) \n",
    "        return trg_mask.to(self.device)\n",
    "              \n",
    "    def forward(\n",
    "        self, \n",
    "        src: torch.Tensor,  \n",
    "        trg: torch.Tensor,\n",
    "    ) -> torch.Tensor:\n",
    "        src_mask = self.make_src_mask(src)\n",
    "        trg_mask = self.make_src_mask(trg)\n",
    "        encoder_src = self.encoder(src, src_mask)\n",
    "        out = self.decoder(trg, encoder_src, src_mask, trg_mask)\n",
    "        return out  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data preprocessing and training\n",
    "I'll be translating from german (src) to english (trg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy \n",
    "from torchtext.data.utils import get_tokenizer\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "from torchtext.datasets import multi30k, Multi30k\n",
    "from torch.utils.data import DataLoader\n",
    "from typing import Iterable, List\n",
    "\n",
    "# !python -m spacy download de\n",
    "# !python -m spacy download en  # run these if you haven't downloaded these languages from spacy before \n",
    "\n",
    "# We need to modify the URLs for the dataset since the links to the original dataset are broken\n",
    "# Refer to https://github.com/pytorch/text/issues/1756#issuecomment-1163664163 for more info\n",
    "multi30k.URL[\"train\"] = \"https://raw.githubusercontent.com/neychev/small_DL_repo/master/datasets/Multi30k/training.tar.gz\"\n",
    "multi30k.URL[\"valid\"] = \"https://raw.githubusercontent.com/neychev/small_DL_repo/master/datasets/Multi30k/validation.tar.gz\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Temirkul\\anaconda3\\envs\\temirkul\\lib\\site-packages\\torch\\utils\\data\\datapipes\\iter\\combining.py:297: UserWarning: Some child DataPipes are not exhausted when __iter__ is called. We are resetting the buffer and each child DataPipe will read from the start again.\n",
      "  warnings.warn(\"Some child DataPipes are not exhausted when __iter__ is called. We are resetting \"\n"
     ]
    }
   ],
   "source": [
    "tokenize_ger = get_tokenizer('spacy', language='de_core_news_sm')\n",
    "tokenize_eng = get_tokenizer('spacy', language='en_core_web_sm')\n",
    "\n",
    "def yield_tokens(data: Iterable, language: str):\n",
    "    for sample_ger, sample_eng in data: \n",
    "        if language == 'de':\n",
    "            yield tokenize_ger(sample_ger)\n",
    "        \n",
    "        elif language == 'en':  \n",
    "            yield tokenize_eng(sample_eng)\n",
    "        \n",
    "train_data, valid_data = Multi30k(split=('train', 'valid'), language_pair=('de', 'en'))\n",
    "\n",
    "# Define special symbols and indices\n",
    "unk_idx, pad_idx, sos_idx, eos_idx = 0, 1, 2, 3\n",
    "# Make sure the tokens are in order of their indices to properly insert them in vocab\n",
    "special_symbols = ['<unk>', '<pad>', '<sos>', '<eos>']\n",
    "vocab_ger = build_vocab_from_iterator(yield_tokens(train_data, 'de'), min_freq=1, specials=special_symbols, special_first=True)\n",
    "vocab_eng = build_vocab_from_iterator(yield_tokens(train_data, 'en'), min_freq=1, specials=special_symbols, special_first=True)\n",
    "\n",
    "# Set ``UNK_IDX`` as the default index. This index is returned when the token is not found.\n",
    "# If not set, it throws ``RuntimeError`` when the queried token is not found in the Vocabulary.\n",
    "vocab_ger.set_default_index(unk_idx)\n",
    "vocab_eng.set_default_index(unk_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "# function to add SOS/EOS and create tensor for input sequence indices\n",
    "def tensor_transform(token_ids: List[int]):\n",
    "    return torch.cat((torch.tensor([sos_idx]), torch.tensor(token_ids), torch.tensor([eos_idx])))\n",
    "\n",
    "def custom_collate_fn(batch): \n",
    "    batch_ger = []  # this is the source  \n",
    "    batch_eng = []  # this is the target \n",
    "    for sample_ger, sample_eng in batch: \n",
    "        sample_ger_tokens = tokenize_ger(sample_ger)  # tokenization \n",
    "        sample_ger_token_ids = vocab_ger(sample_ger_tokens)  # converting tokens into ids \n",
    "        batch_ger.append(tensor_transform(sample_ger_token_ids))\n",
    "        \n",
    "        sample_eng_tokens = tokenize_eng(sample_eng)  # tokenization \n",
    "        sample_eng_token_ids = vocab_eng(sample_eng_tokens)  # converting tokens into ids \n",
    "        batch_eng.append(tensor_transform(sample_eng_token_ids))\n",
    "    \n",
    "    batch_ger = pad_sequence(batch_ger, padding_value=pad_idx, batch_first=True)\n",
    "    batch_eng = pad_sequence(batch_eng, padding_value=pad_idx, batch_first=True)\n",
    "    return batch_ger, batch_eng"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "# device \n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)\n",
    "\n",
    "# training hyperparams \n",
    "num_epochs = 150\n",
    "learning_rate = 1e-6\n",
    "batch_size = 8\n",
    "\n",
    "# model hyperparams \n",
    "src_vocab_size = len(vocab_ger)\n",
    "trg_vocab_size = len(vocab_eng)\n",
    "embed_dim = 512 \n",
    "n_layers = 3\n",
    "n_heads = 4\n",
    "forward_expansion = 4\n",
    "dropout = 0 \n",
    "max_seq_length = 50\n",
    "\n",
    "model = Translator(\n",
    "    src_vocab_size=src_vocab_size,\n",
    "    trg_vocab_size=trg_vocab_size,\n",
    "    src_pad_idx=pad_idx,\n",
    "    trg_pad_idx=pad_idx,  \n",
    "    embed_dim=embed_dim,\n",
    "    n_layers=n_layers,\n",
    "    n_heads=n_heads,\n",
    "    device=device,\n",
    "    forward_expansion=forward_expansion,\n",
    "    dropout=dropout,\n",
    "    max_seq_length=max_seq_length\n",
    ").to(device)\n",
    "\n",
    "train_dataloader = DataLoader(dataset=train_data, batch_size=batch_size, collate_fn=custom_collate_fn, shuffle=True)\n",
    "valid_dataloader = DataLoader(dataset=valid_data, batch_size=batch_size, collate_fn=custom_collate_fn, shuffle=True)\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "ce_loss = nn.CrossEntropyLoss(ignore_index=pad_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# useful functions \n",
    "def valid_loss_fn(model, valid_dataloader, valid_dataloader_length: int, loss_function, device):\n",
    "    model.eval()\n",
    "    loss = 0\n",
    "    for batch_idx, (batch_ger, batch_eng) in enumerate(valid_dataloader): \n",
    "        batch_ger = batch_ger.to(device).to(torch.int64)\n",
    "        batch_eng = batch_eng.to(device).to(torch.int64)\n",
    "        \n",
    "        output = model(batch_ger, batch_eng[:, :-1])\n",
    "        loss += loss_function(torch.permute(output, (0, 2, 1)), batch_eng[:, 1:])\n",
    "    model.train()\n",
    "    return loss/valid_dataloader_length\n",
    "    \n",
    "def translate_sentence(model, sentence: str, max_length, device):  # translation is done using greedy decoding. Note that max_length must be less than max_seq_length\n",
    "    model.eval()\n",
    "    sentence = tensor_transform(vocab_ger(tokenize_ger(sentence))).unsqueeze(0).to(device)  # i also need to add a singleton batch dimension\n",
    "    decoder_inputs_list = [sos_idx]\n",
    "    decoder_inputs = torch.tensor(decoder_inputs_list).unsqueeze(0).to(device)\n",
    "    for i in range(max_length - 1):\n",
    "        output = model(sentence, decoder_inputs)  # shape [1, decoder input seq length, vocab size]\n",
    "        next_word = torch.tensor([torch.argmax(output[0, -1, :])]).unsqueeze(0).to(device)  # select the output from the last element of decoder the sequence to get the next word \n",
    "        decoder_inputs = torch.cat((decoder_inputs, next_word), dim=1)\n",
    "        if next_word == eos_idx:\n",
    "            break\n",
    "        \n",
    "    token_ids = decoder_inputs[0, :].tolist()\n",
    "    itos = vocab_eng.get_itos()\n",
    "    tokens = [itos[token] for token in token_ids]\n",
    "    model.train()\n",
    "    return tokens  # returns a list of tokens "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8, 18]) torch.Size([8, 17])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Temirkul\\anaconda3\\envs\\temirkul\\lib\\site-packages\\torchinfo\\torchinfo.py:462: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  action_fn=lambda data: sys.getsizeof(data.storage()),\n",
      "c:\\Users\\Temirkul\\anaconda3\\envs\\temirkul\\lib\\site-packages\\torch\\storage.py:665: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return super().__sizeof__() + self.nbytes()\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "========================================================================================================================\n",
       "Layer (type:depth-idx)                        Input Shape               Output Shape              Param #\n",
       "========================================================================================================================\n",
       "Translator                                    [8, 18]                   [8, 17, 10837]            --\n",
       "├─Encoder: 1-1                                [8, 18]                   [8, 18, 512]              --\n",
       "│    └─Embedding: 2-1                         [8, 18]                   [8, 18, 512]              9,837,568\n",
       "│    └─Embedding: 2-2                         [8, 18]                   [8, 18, 512]              25,600\n",
       "│    └─Dropout: 2-3                           [8, 18, 512]              [8, 18, 512]              --\n",
       "│    └─ModuleList: 2-4                        --                        --                        --\n",
       "│    │    └─TransformerBlock: 3-1             [8, 18, 512]              [8, 18, 512]              3,149,824\n",
       "│    │    └─TransformerBlock: 3-2             [8, 18, 512]              [8, 18, 512]              3,149,824\n",
       "│    │    └─TransformerBlock: 3-3             [8, 18, 512]              [8, 18, 512]              3,149,824\n",
       "├─Decoder: 1-2                                [8, 17]                   [8, 17, 10837]            --\n",
       "│    └─Embedding: 2-5                         [8, 17]                   [8, 17, 512]              5,548,544\n",
       "│    └─Embedding: 2-6                         [8, 17]                   [8, 17, 512]              25,600\n",
       "│    └─Dropout: 2-7                           [8, 17, 512]              [8, 17, 512]              --\n",
       "│    └─ModuleList: 2-8                        --                        --                        --\n",
       "│    │    └─DecoderBlock: 3-4                 [8, 17, 512]              [8, 17, 512]              4,199,936\n",
       "│    │    └─DecoderBlock: 3-5                 [8, 17, 512]              [8, 17, 512]              4,199,936\n",
       "│    │    └─DecoderBlock: 3-6                 [8, 17, 512]              [8, 17, 512]              4,199,936\n",
       "│    └─Linear: 2-9                            [8, 17, 512]              [8, 17, 10837]            5,559,381\n",
       "========================================================================================================================\n",
       "Total params: 43,045,973\n",
       "Trainable params: 43,045,973\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (M): 344.42\n",
       "========================================================================================================================\n",
       "Input size (MB): 0.00\n",
       "Forward/backward pass size (MB): 57.04\n",
       "Params size (MB): 172.18\n",
       "Estimated Total Size (MB): 229.23\n",
       "========================================================================================================================"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# model summary \n",
    "from torchinfo import summary \n",
    "input_data = list(train_dataloader)[0]\n",
    "print(input_data[0].shape, input_data[1].shape)\n",
    "summary(model, input_data=[input_data[0], input_data[1]], device=device, col_names=('input_size', 'output_size', 'num_params'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 2 torch.Size([8, 21]) torch.Size([8, 24])\n",
      "1 2 torch.Size([8, 19]) torch.Size([8, 18])\n",
      "2 2 torch.Size([8, 22]) torch.Size([8, 21])\n",
      "3 2 torch.Size([8, 19]) torch.Size([8, 18])\n",
      "4 2 torch.Size([8, 21]) torch.Size([8, 20])\n",
      "5 2 torch.Size([8, 21]) torch.Size([8, 19])\n",
      "0 2 torch.Size([8, 33]) torch.Size([8, 33])\n",
      "1 2 torch.Size([8, 18]) torch.Size([8, 21])\n",
      "2 2 torch.Size([8, 21]) torch.Size([8, 20])\n",
      "3 2 torch.Size([8, 23]) torch.Size([8, 22])\n",
      "4 2 torch.Size([8, 24]) torch.Size([8, 25])\n",
      "5 2 torch.Size([8, 19]) torch.Size([8, 20])\n",
      "3626 127\n"
     ]
    }
   ],
   "source": [
    "for batch_idx, batch in enumerate(train_dataloader):  \n",
    "    print(batch_idx, len(batch), batch[0].shape, batch[1].shape)\n",
    "    if batch_idx == 5: \n",
    "        break \n",
    "train_dataloader_length = len(list(train_dataloader))\n",
    "\n",
    "for batch_idx, batch in enumerate(valid_dataloader):  \n",
    "    print(batch_idx, len(batch), batch[0].shape, batch[1].shape)\n",
    "    if batch_idx == 5: \n",
    "        break \n",
    "valid_dataloader_length = len(list(valid_dataloader))\n",
    "\n",
    "print(train_dataloader_length, valid_dataloader_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<sos>', 'A', 'man', 'in', 'a', 'in', 'a', 'a', 'a', 'a', '.', '<eos>']\n",
      "Epoch 0\n",
      "['<sos>', 'Two', 'men', 'in', 'in', 'the', 'in', 'the', 'in', 'in', 'the', 'in', 'the', 'in', 'the', 'in', 'the', 'in', 'the', 'in', 'the', 'in', 'the', 'in', 'the', '<eos>']\n",
      "Epoch 1\n",
      "['<sos>', '.', '<eos>']\n",
      "Epoch 2\n",
      "['<sos>', '.', '<eos>']\n",
      "Epoch 3\n",
      "['<sos>', '.', '<eos>']\n",
      "Epoch 4\n",
      "['<sos>', '.', '<eos>']\n",
      "Epoch 5\n",
      "['<sos>', '.', '<eos>']\n",
      "Epoch 6\n",
      "['<sos>', '.', '<eos>']\n",
      "Epoch 7\n",
      "['<sos>', '.', '<eos>']\n",
      "Epoch 8\n",
      "['<sos>', '.', '<eos>']\n",
      "Epoch 9\n",
      "['<sos>', '.', '<eos>']\n",
      "Epoch 10\n",
      "['<sos>', '.', '<eos>']\n",
      "Epoch 11\n",
      "['<sos>', '.', '<eos>']\n",
      "Epoch 12\n",
      "['<sos>', '.', '<eos>']\n",
      "Epoch 13\n",
      "['<sos>', '.', '<eos>']\n",
      "Epoch 14\n",
      "['<sos>', '.', '<eos>']\n",
      "Epoch 15\n",
      "['<sos>', '.', '<eos>']\n",
      "Epoch 16\n",
      "['<sos>', '.', '<eos>']\n",
      "Epoch 17\n",
      "['<sos>', '.', '<eos>']\n",
      "Epoch 18\n",
      "['<sos>', '.', '<eos>']\n",
      "Epoch 19\n",
      "['<sos>', '.', '<eos>']\n",
      "Epoch 20\n",
      "['<sos>', '.', '<eos>']\n",
      "Epoch 21\n",
      "['<sos>', '.', '<eos>']\n",
      "Epoch 22\n",
      "['<sos>', '.', '<eos>']\n",
      "Epoch 23\n",
      "['<sos>', '.', '<eos>']\n",
      "Epoch 24\n",
      "['<sos>', '.', '<eos>']\n",
      "Epoch 25\n",
      "['<sos>', '.', '<eos>']\n",
      "Epoch 26\n",
      "['<sos>', '.', '<eos>']\n",
      "Epoch 27\n",
      "['<sos>', '.', '<eos>']\n",
      "Epoch 28\n",
      "['<sos>', '.', '<eos>']\n",
      "Epoch 29\n",
      "['<sos>', '.', '<eos>']\n",
      "Epoch 30\n",
      "['<sos>', '.', '<eos>']\n",
      "Epoch 31\n",
      "['<sos>', '.', '<eos>']\n",
      "Epoch 32\n",
      "['<sos>', '.', '<eos>']\n",
      "Epoch 33\n",
      "['<sos>', '.', '<eos>']\n",
      "Epoch 34\n",
      "['<sos>', '.', '<eos>']\n",
      "Epoch 35\n",
      "['<sos>', '.', '<eos>']\n",
      "Epoch 36\n",
      "['<sos>', '.', '<eos>']\n",
      "Epoch 37\n",
      "['<sos>', '.', '<eos>']\n",
      "Epoch 38\n",
      "['<sos>', '.', '<eos>']\n",
      "Epoch 39\n",
      "['<sos>', '.', '<eos>']\n",
      "Epoch 40\n",
      "['<sos>', '.', '<eos>']\n",
      "Epoch 41\n",
      "['<sos>', '.', '<eos>']\n",
      "Epoch 42\n",
      "['<sos>', '.', '<eos>']\n",
      "Epoch 43\n",
      "['<sos>', '.', '<eos>']\n",
      "Epoch 44\n",
      "['<sos>', '.', '<eos>']\n",
      "Epoch 45\n",
      "['<sos>', '.', '<eos>']\n",
      "Epoch 46\n",
      "['<sos>', '.', '<eos>']\n",
      "Epoch 47\n",
      "['<sos>', '.', '<eos>']\n",
      "Epoch 48\n",
      "['<sos>', '.', '<eos>']\n",
      "Epoch 49\n",
      "['<sos>', '.', '<eos>']\n",
      "Epoch 50\n",
      "['<sos>', '.', '<eos>']\n",
      "Epoch 51\n",
      "['<sos>', '.', '<eos>']\n",
      "Epoch 52\n",
      "['<sos>', '.', '<eos>']\n",
      "Epoch 53\n",
      "['<sos>', '.', '<eos>']\n",
      "Epoch 54\n",
      "['<sos>', '.', '<eos>']\n",
      "Epoch 55\n",
      "['<sos>', '.', '<eos>']\n",
      "Epoch 56\n",
      "['<sos>', '.', '<eos>']\n",
      "Epoch 57\n",
      "['<sos>', '.', '<eos>']\n",
      "Epoch 58\n",
      "['<sos>', '.', '<eos>']\n",
      "Epoch 59\n",
      "['<sos>', '.', '<eos>']\n",
      "Epoch 60\n",
      "['<sos>', '.', '<eos>']\n",
      "Epoch 61\n",
      "['<sos>', '.', '<eos>']\n",
      "Epoch 62\n",
      "['<sos>', '.', '<eos>']\n",
      "Epoch 63\n",
      "['<sos>', '.', '<eos>']\n",
      "Epoch 64\n",
      "['<sos>', '.', '<eos>']\n",
      "Epoch 65\n",
      "['<sos>', '.', '<eos>']\n",
      "Epoch 66\n",
      "['<sos>', '.', '<eos>']\n",
      "Epoch 67\n",
      "['<sos>', '.', '<eos>']\n",
      "Epoch 68\n",
      "['<sos>', 'a', '<eos>']\n",
      "Epoch 69\n",
      "['<sos>', '.', '<eos>']\n",
      "Epoch 70\n",
      "['<sos>', 'a', '<eos>']\n",
      "Epoch 71\n",
      "['<sos>', 'a', '<eos>']\n",
      "Epoch 72\n",
      "['<sos>', '.', '<eos>']\n",
      "Epoch 73\n",
      "['<sos>', 'a', '<eos>']\n",
      "Epoch 74\n",
      "['<sos>', '.', '<eos>']\n",
      "Epoch 75\n",
      "['<sos>', '.', '<eos>']\n",
      "Epoch 76\n",
      "['<sos>', 'a', '<eos>']\n",
      "Epoch 77\n",
      "['<sos>', 'a', '<eos>']\n",
      "Epoch 78\n",
      "['<sos>', 'a', '<eos>']\n",
      "Epoch 79\n",
      "['<sos>', 'a', '<eos>']\n",
      "Epoch 80\n",
      "['<sos>', 'a', '<eos>']\n",
      "Epoch 81\n",
      "['<sos>', 'a', '<eos>']\n",
      "Epoch 82\n",
      "['<sos>', 'a', '<eos>']\n",
      "Epoch 83\n",
      "['<sos>', 'a', '<eos>']\n",
      "Epoch 84\n",
      "['<sos>', 'a', '<eos>']\n",
      "Epoch 85\n",
      "['<sos>', 'a', '<eos>']\n",
      "Epoch 86\n",
      "['<sos>', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', '<eos>']\n",
      "Epoch 87\n",
      "['<sos>', 'a', '<eos>']\n",
      "Epoch 88\n",
      "['<sos>', 'a', '<eos>']\n",
      "Epoch 89\n",
      "['<sos>', 'a', '<eos>']\n",
      "Epoch 90\n",
      "['<sos>', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', '<eos>']\n",
      "Epoch 91\n",
      "['<sos>', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', '<eos>']\n",
      "Epoch 92\n",
      "['<sos>', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', '<eos>']\n",
      "Epoch 93\n",
      "['<sos>', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', '<eos>']\n",
      "Epoch 94\n",
      "['<sos>', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', '<eos>']\n",
      "Epoch 95\n",
      "['<sos>', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', '<eos>']\n",
      "Epoch 96\n",
      "['<sos>', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', '<eos>']\n",
      "Epoch 97\n",
      "['<sos>', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', '<eos>']\n",
      "Epoch 98\n",
      "['<sos>', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', '<eos>']\n",
      "Epoch 99\n",
      "['<sos>', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', '<eos>']\n",
      "Epoch 100\n",
      "['<sos>', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', '<eos>']\n",
      "Epoch 101\n",
      "['<sos>', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', '<eos>']\n",
      "Epoch 102\n",
      "['<sos>', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', '<eos>']\n",
      "Epoch 103\n",
      "['<sos>', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', '<eos>']\n",
      "Epoch 104\n",
      "['<sos>', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', '<eos>']\n",
      "Epoch 105\n",
      "['<sos>', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', '<eos>']\n",
      "Epoch 106\n",
      "['<sos>', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', '<eos>']\n",
      "Epoch 107\n",
      "['<sos>', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', '<eos>']\n",
      "Epoch 108\n",
      "['<sos>', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', '<eos>']\n",
      "Epoch 109\n",
      "['<sos>', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', '<eos>']\n",
      "Epoch 110\n",
      "['<sos>', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', '<eos>']\n",
      "Epoch 111\n",
      "['<sos>', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', '<eos>']\n",
      "Epoch 112\n",
      "['<sos>', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', '<eos>']\n",
      "Epoch 113\n",
      "['<sos>', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', '<eos>']\n",
      "Epoch 114\n",
      "['<sos>', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', '<eos>']\n",
      "Epoch 115\n",
      "['<sos>', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', '<eos>']\n",
      "Epoch 116\n",
      "['<sos>', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', '<eos>']\n",
      "Epoch 117\n",
      "['<sos>', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', '<eos>']\n",
      "Epoch 118\n",
      "['<sos>', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', '<eos>']\n",
      "Epoch 119\n",
      "['<sos>', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', '<eos>']\n",
      "Epoch 120\n",
      "['<sos>', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', '<eos>']\n",
      "Epoch 121\n",
      "['<sos>', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', '<eos>']\n",
      "Epoch 122\n",
      "['<sos>', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', '<eos>']\n",
      "Epoch 123\n",
      "['<sos>', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', '<eos>']\n",
      "Epoch 124\n",
      "['<sos>', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', '<eos>']\n",
      "Epoch 125\n",
      "['<sos>', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', '<eos>']\n",
      "Epoch 126\n",
      "['<sos>', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', '<eos>']\n",
      "Epoch 127\n",
      "['<sos>', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', '<eos>']\n",
      "Epoch 128\n",
      "['<sos>', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', '<eos>']\n",
      "Epoch 129\n",
      "['<sos>', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', '<eos>']\n",
      "Epoch 130\n",
      "['<sos>', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', '<eos>']\n",
      "Epoch 131\n",
      "['<sos>', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', '<eos>']\n",
      "Epoch 132\n",
      "['<sos>', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', '<eos>']\n",
      "Epoch 133\n",
      "['<sos>', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', '<eos>']\n",
      "Epoch 134\n",
      "['<sos>', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', '<eos>']\n",
      "Epoch 135\n",
      "['<sos>', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', '<eos>']\n",
      "Epoch 136\n",
      "['<sos>', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', '<eos>']\n",
      "Epoch 137\n",
      "['<sos>', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', '<eos>']\n",
      "Epoch 138\n",
      "['<sos>', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', '<eos>']\n",
      "Epoch 139\n",
      "['<sos>', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', '<eos>']\n",
      "Epoch 140\n",
      "['<sos>', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', '<eos>']\n",
      "Epoch 141\n",
      "['<sos>', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', '<eos>']\n",
      "Epoch 142\n",
      "['<sos>', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', '<eos>']\n",
      "Epoch 143\n",
      "['<sos>', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', '<eos>']\n",
      "Epoch 144\n",
      "['<sos>', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', '<eos>']\n",
      "Epoch 145\n",
      "['<sos>', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', '<eos>']\n",
      "Epoch 146\n",
      "['<sos>', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', '<eos>']\n",
      "Epoch 147\n",
      "['<sos>', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', '<eos>']\n",
      "Epoch 148\n",
      "['<sos>', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', '<eos>']\n",
      "Epoch 149\n"
     ]
    }
   ],
   "source": [
    "# training \n",
    "train_loss = np.zeros(num_epochs)\n",
    "valid_loss = np.zeros(num_epochs)\n",
    "example_sentence = \"Polizist ringt mit einem Mann, der eine Hand in die Luft streckt, während andere Polizisten zusehen.\"\n",
    "reference_translation = \"Cop wrestle one man with his hand sticking up in the air as other cops watch.\"\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    temp_arr = np.zeros(train_dataloader_length)\n",
    "    for batch_idx, (batch_ger, batch_eng) in enumerate(train_dataloader):\n",
    "        batch_ger = batch_ger.to(device).to(torch.int64)\n",
    "        batch_eng = batch_eng.to(device).to(torch.int64)\n",
    "        \n",
    "        output = model(batch_ger, batch_eng[:, :-1])  # exclude the <eos> token from the decoder input (because we want to predict it)\n",
    "        output = output.reshape(-1, output.shape[2])\n",
    "        loss = ce_loss(output, batch_eng[:, 1:].reshape(-1))  # shift labels by 1 to the right\n",
    "        temp_arr[batch_idx] = loss \n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    train_loss[epoch] = np.mean(temp_arr)\n",
    "    valid_loss[epoch] = valid_loss_fn(model, valid_dataloader, valid_dataloader_length, ce_loss, device)\n",
    "    \n",
    "    print(translate_sentence(model, example_sentence, 50, device))\n",
    "    print(f\"Epoch {epoch}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<sos>', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', '<eos>']\n"
     ]
    }
   ],
   "source": [
    "train_example_sentence = \"Ein kleiner Junge mit einer blauen Baseballkappe sitzt auf einem Dock und blickt auf das Wasser.\"  # an example from the training set \n",
    "train_reference_translation = \"A little boy wearing a blue baseball cap is sitting on a dock overlooking the water.\"\n",
    "print(translate_sentence(model, train_example_sentence, 50, device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.plot(np.arange(num_epochs), valid_loss, label='validation')\n",
    "plt.plot(np.arange(num_epochs), train_loss, label='train')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "# plt.ylim(0, 1)\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "temirkul",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
